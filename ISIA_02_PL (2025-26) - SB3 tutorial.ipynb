{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Reinforcement Learning in Python with Stable Baselines 3\n",
    "\n",
    "_from_ [https://pythonprogramming.net/introduction-reinforcement-learning-stable-baselines-3-tutorial/](https://pythonprogramming.net/introduction-reinforcement-learning-stable-baselines-3-tutorial/)\n",
    "\n",
    "_updated for Gymnasium_"
   ],
   "id": "142a3f555cc09451"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Welcome to a tutorial series covering how to do reinforcement learning with the Stable-Baselines3 (SB3) package. The objective of the SB3 library is to be for reinforcement learning like what sklearn is for general machine learning. Stable-Baselines3 enables you to try things like PPO, then maybe some TD3 and then why not try some SAC?!\n",
    "\n",
    "The SB3 website contains links and resources to the original papers for algorithms and even some followup information in many cases. There's really no need for you to know every single RL algorithm, just to try them, and it's completely reasonable to try some algorithms on some problem, then do more research into the ones that seem promising to you. To start, here are some quick words and definitions that you're likely to come across:\n",
    "\n",
    "- The Environment: What are you trying to solve? (cartpole, lunar lander, some other custom environment). If you're trying to make some AI play a game, the game is the environment.\n",
    "- The Model: What algorithm are you using (PPO, SAC, TRPO, TD3...etc).\n",
    "- The Agent: this is the thing that interacts with the environment using an algorithm/model.\n",
    "\n",
    "Then, looking closer at the environments, you have two major elements:\n",
    "\n",
    "- The Observation (or \"state\"): What is the state of the environment? This could be imagery/visuals, or just vector information. For example, your observation in cartpole is the angle and velocity of the pole. In the bipedal walker environment, the observation contains readings from lidar, the hull's angle, leg positions...etc. An observation is all of this information at some point in time. The observation space is a description, mainly the shape, of those observations.\n",
    "- The Action: What are the options for your agent in this environment? For example, in cartpole, you can either push left or right. The action space is a description of these possible actions, both in terms of the shape and type (discrete or continuous).\n",
    "- Step: Take a step in the environment. In general, you pass your action to the step method, the environment performs the step and returns a new observation and reward. You can think of this like frames per second. If you're playing 30 frames per second, then you could have 30 steps per second, but it can get far more complicated than this. A step is just simply progressing in the environment.\n",
    "\n",
    "What's discrete and continuous?\n",
    "\n",
    "- Discrete: Think of discrete like classifications. Cartpole has a discrete action space. You can go left, or right. Nothing in between, there's no concept of sort of left or half left. It's left, or right.\n",
    "- Continuous: Think of continuous like regression. It's a range of nearly infinite possibilities. The bidpedal walker environement is a continuous action space, because you set the servo torque anywhere in a range between -1 and positive 1.\n",
    "\n",
    "Typically, a continuous environment is harder to learn, but sometimes this is a requirement. In robotics, servos/motors of decent quality tend to be naturally largely continuous. Even servos, however, are *actually* discrete, with something like 32,768 positions, but this large of a discrete space is, typically, far too big for a discrete space. That said, there are ways to convert continuous spaces to discrete, in effort to make training faster and easier, and maybe more on that later on. For now, it's just important to understand the two major types of action spaces, as well as the general description of environments, agents, and models.\n",
    "\n",
    "Once you find some algorithm that seems to be maybe working and learning something, you should dive more deeply into that model, how it works, and the various hyperparameter options that you can tweak.\n",
    "\n",
    "With all of that out of the way, let's play! To start, you will need Pytorch and stable-baselines3. For Pytorch, just follow the instructions here: [Pytorch getting started](https://pytorch.org/get-started/locally/) (https://pytorch.org/get-started/locally/). For stable-baselines3: `pip3 install stable-baselines3[extra]`. Finally, we'll need some environments to learn on, for this we'll use Gymnasium, which you can get with `pip install gymnasium[box2d]`. On Linux for Gymnasium and the box2d environments, you also needed to do the following:\n",
    "\n",
    "```bash\n",
    "    apt install xvfb ffmpeg xorg-dev libsdl2-dev swig cmake\n",
    "    pip install gymnasium[box2d]\n",
    "```\n",
    "\n",
    "For this tutorial, we'll start with the lunar lander environment. First, let's get a grasp of the fundamentals of our environment. When choosing algorithms to try, or creating your own environment, you will need to start thinking in terms of observations and actions, per step. The structure of a Farama Gymnasium problem is the standard by which basically everyone does reinforcement learning. Let's take a peak at the lunar lander environment:\n"
   ],
   "id": "1829e5f0447b29a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('LunarLander-v3', render_mode='human')  # continuous: LunarLanderContinuous-v3\n",
    "\n",
    "# required before you can step the environment\n",
    "env.reset(seed=42)\n",
    "\n",
    "# sample action:\n",
    "print(\"sample action:\", env.action_space.sample())\n",
    "\n",
    "# observation space shape:\n",
    "print(\"observation space shape:\", env.observation_space.shape)\n",
    "\n",
    "# sample observation:\n",
    "print(\"sample observation:\", env.observation_space.sample())\n",
    "\n",
    "env.close()"
   ],
   "id": "36b4b7197aac4587",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Running that, you should see something like:\n",
    "\n",
    "```\n",
    "sample action: 2\n",
    "observation space shape: (8,)\n",
    "sample observation: [-0.51052296  0.00194223  1.4957197  -0.3037317  -0.20905018 -0.1737924\n",
    "    1.8414629   0.09498857]\n",
    "```\n",
    "\n",
    "\n",
    "Our actions are discrete, and just 1 discrete action of 0, 1, 2, or 3. 0 means do nothing, 1 means fire the left engine, 2 means fire the bottom engine, and 3 means fire the right engine. The observation space is a vector of 8 values. Looking at the gymnasium code for this environment, we can find out what the values in the observation mean https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/box2d/lunar_lander.py#L800:\n",
    "\n",
    "```\n",
    "     env: The environment\n",
    "s (list): The state. Attributes:\n",
    "            s[0] is the horizontal coordinate\n",
    "            s[1] is the vertical coordinate\n",
    "            s[2] is the horizontal speed\n",
    "            s[3] is the vertical speed\n",
    "            s[4] is the angle\n",
    "            s[5] is the angular speed\n",
    "            s[6] 1 if first leg has contact, else 0\n",
    "            s[7] 1 if second leg has contact, else 0\n",
    "\n",
    "```\n",
    "\n",
    "We can see a sample of the environment by running:"
   ],
   "id": "50e2086c0aa6ab20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "env = gym.make('LunarLander-v3', render_mode=\"human\")  # continuous: LunarLanderContinuous-v3\n",
    "env.reset(seed=42)\n",
    "\n",
    "for step in range(200):\n",
    "    env.render()\n",
    "    # take random action\n",
    "    env.step(env.action_space.sample())\n",
    "\n",
    "env.close()"
   ],
   "id": "a6ba32f87b2a96bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "That doesn't look too good, but it's some an agent acting randomly, using `env.action_space.sample()`. Each time we step in the environment, the step method also returns some information to us. We can collect it with:\n",
    "\n",
    "```python\n",
    "observation, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "```\n",
    "\n",
    "Here, we're gathering the observation, reward, whether or not the environment has reported that it's done, and any other extra info. The observation will be those 8 values listed above, the reward is a value reported by the environment, meant to be some sort of signal of how well the agent is accomplishing the desired objective. In the case of the lunar lander, the goal is to land between the two flags. Let's check out the reward and done values:"
   ],
   "id": "ac748690ce9f110b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "env = gym.make('LunarLander-v3', render_mode=\"human\")  # continuous: LunarLanderContinuous-v3\n",
    "env.reset(seed=42)\n",
    "\n",
    "for step in range(200):\n",
    "    env.render()\n",
    "\n",
    "    # take random action\n",
    "    observation, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "    done = terminated or truncated\n",
    "\n",
    "    print(reward, done)\n",
    "\n",
    "env.close()"
   ],
   "id": "e094163893e434b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```text\n",
    "-8.966428059751639 False\n",
    "-3.2056261702008144 False\n",
    "-7.918002269808301 False\n",
    "-5.045565371482126 False\n",
    "-4.492306794371302 False\n",
    "-12.056824418777229 False\n",
    "-8.002752408838138 False\n",
    "-11.950438693580214 False\n",
    "-10.814724683236523 False\n",
    "-4.34849509508271 False\n",
    "4.965781267653142 False\n",
    "-5.928775063421142 False\n",
    "-100 True\n",
    "-100 True\n",
    "-100 True\n",
    "-100 True\n",
    "-100 True\n",
    "-100 True\n",
    "-100 True\n",
    "```\n",
    "\n",
    "As you can see, we got some varying rewards, and then eventually got a bunch of negative -100s and True for the environment being done. This is the environment knowing we crashed.\n",
    "\n",
    "The objective of reinforcement learning is essentially to keep playing in some environment with the goal of seeking out better and better rewards. So now, we need an algorithm that can solve this environment. This tends to boil down to the action space. Which algorithms can support our action space? What is our action space? It's a single discrete value.\n",
    "\n",
    "From https://stable-baselines3.readthedocs.io/en/master/guide/algos.html:\n",
    "\n",
    "| Name                       | `Box` | `Discrete` | `MultiDiscrete` | `MultiBinary` | Multi Processing |\n",
    "|----------------------------| ----- | ---------- | --------------- | ------------- | ---------------- |\n",
    "| ARS <sup>1</sup>           | ✔️     | ✔️          | ❌               | ❌             | ✔️                |\n",
    "| A2C                        | ✔️     | ✔️          | ✔️               | ✔️             | ✔️                |\n",
    "| CrossQ <sup>1</sup>        | ✔️     | ❌          | ❌               | ❌             | ✔️                |\n",
    "| DDPG                       | ✔️     | ❌          | ❌               | ❌             | ✔️                |\n",
    "| DQN                        | ❌     | ✔️          | ❌               | ❌             | ✔️                |\n",
    "| HER                        | ✔️     | ✔️          | ❌               | ❌             | ✔️                |\n",
    "| PPO                        | ✔️     | ✔️          | ✔️               | ✔️             | ✔️                |\n",
    "| QR-DQN <sup>1</sup>        | ❌     | ️ ✔️         | ❌               | ❌             | ✔️                |\n",
    "| RecurrentPPO <sup>1</sup>  | ✔️     | ✔️          | ✔️               | ✔️             | ✔️                |\n",
    "| SAC                        | ✔️     | ❌          | ❌               | ❌             | ✔️                |\n",
    "| TD3                        | ✔️     | ❌          | ❌               | ❌             | ✔️                |\n",
    "| TQC <sup>1</sup>           | ✔️     | ❌          | ❌               | ❌             | ✔️                |\n",
    "| TRPO  <sup>1</sup>         | ✔️     | ✔️          | ✔️               | ✔️             | ✔️                |\n",
    "| Maskable PPO <sup>1</sup>  | ❌     | ✔️          | ✔️               | ✔️             | ✔️                |\n",
    "\n",
    "<sup>1</sup> Implemented in [SB3 Contrib](https://github.com/Stable-Baselines-Team/stable-baselines3-contrib)\n",
    "\n",
    "This table gives the algorithms available in SB3, along with the action spaces that they support, and multiprocessing. When you have some problem, you can start by consulting this table to see which algorithms you can start off by trying. You should recognize discrete, but then there are things like \"MultiDiscrete\" and \"Box\" and \"MultiBinary.\" What are those? Where's continuous?\n",
    "\n",
    "- Box: For now, you can think of this as your continuous space support.\n",
    "\n",
    "- MultiDiscrete: some environments are just one discrete action, but some environments may have multiple discrete actions to take.\n",
    "\n",
    "- MultiBinary: Similar to MultiDiscrete, but for instances where there are only 2 options for each action\n",
    "\n",
    "It looks like we have quite a few options to try: A2C, DQN, HER, PPO, QRDQN, and maskable PPO. There may be even more algorithms available later, so be sure to check out the [SB3 algorithms](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html) (https://stable-baselines3.readthedocs.io/en/master/guide/algos.html) page later when working on your own problems. Let's try out the first one on the list: A2C. To start, we'll need to import it:\n",
    "\n",
    "```python\n",
    "from stable_baselines3 import A2C\n",
    "```\n",
    "\n",
    "Then, after we've defined the environment, we'll define the model to run in this environment, and then have it learn for 10,000 timesteps.\n",
    "\n",
    "```python\n",
    "model = A2C('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "```\n",
    "After we've got a trained model, we probably are keen to see it, so let's check out the results:\n",
    "\n",
    "```python\n",
    "episodes = 10\n",
    "\n",
    "for ep in range(episodes):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        # pass observation to model to get predicted action\n",
    "        action, _states = model.predict(observation)\n",
    "\n",
    "        # pass action to env and get info back\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "\n",
    "        # show the environment on the screen\n",
    "        env.render()\n",
    "```\n",
    "\n",
    "This is a very simple example, but it's a good starting point. We'll see more examples later. Full code up to this point:"
   ],
   "id": "b76797a0bdace113"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "\n",
    "env = gym.make('LunarLander-v3', render_mode='human')  # continuous: LunarLanderContinuous-v3\n",
    "env.reset(seed=42)\n",
    "\n",
    "model = A2C('MlpPolicy', env, verbose=1, device='cpu')\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "for ep in range(episodes):\n",
    "    observation, info = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _states = model.predict(observation)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        env.render()\n",
    "        print(reward)"
   ],
   "id": "75b801f13073be4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Outputs in the terminal will look something like:\n",
    "\n",
    "```text\n",
    "------------------------------------\n",
    "| rollout/              |          |\n",
    "|    ep_len_mean        | 190      |\n",
    "|    ep_rew_mean        | -195     |\n",
    "| time/                 |          |\n",
    "|    fps                | 506      |\n",
    "|    iterations         | 1300     |\n",
    "|    time_elapsed       | 12       |\n",
    "|    total_timesteps    | 6500     |\n",
    "| train/                |          |\n",
    "|    entropy_loss       | -0.575   |\n",
    "|    explained_variance | 0.0149   |\n",
    "|    learning_rate      | 0.0007   |\n",
    "|    n_updates          | 1299     |\n",
    "|    policy_loss        | -3.38    |\n",
    "|    value_loss         | 93       |\n",
    "------------------------------------\n",
    "```\n",
    "\n",
    "This contains a few statistics like how many steps your model has taken as well as probably the thing you care about most, the episode's reward mean: `ep_rew_mean`.\n",
    "\n",
    "Okay, not terrible, but this wasn't enough time apparently to train the agent! Let's try 100,000 steps instead."
   ],
   "id": "7634435431f5ad97"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "\n",
    "env = gym.make('LunarLander-v3', render_mode='human')  # continuous: LunarLanderContinuous-v3\n",
    "env.reset()\n",
    "\n",
    "model = A2C('MlpPolicy', env, verbose=1, device='cpu')\n",
    "model.learn(total_timesteps=100000)\n",
    "\n",
    "episodes = 5\n",
    "\n",
    "for ep in range(episodes):\n",
    "    observation, info = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _states = model.predict(observation)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        env.render()\n",
    "        print(reward)"
   ],
   "id": "60aa285c6725a3a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Hmm, well, at least the lander isn't crashing, but it also is pretty rarely actually landing at a reasonable pace. On a realistic problem, you might start thinking about tweaking the reward a bit to maybe disincentivise floating in place, or maybe you just need to be more patient and do more steps. A2C is a fairly old (in terms of reinforcement learning) algorithm, maybe we'll try something else instead? Let's try PPO. All we need to do is import it:\n",
    "\n",
    "```python\n",
    "from stable_baselines3 import PPO\n",
    "```\n",
    "\n",
    "Then change our model from A2C to PPO:\n",
    "\n",
    "```python\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "```\n"
   ],
   "id": "432fecdddade230f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "\n",
    "env = gym.make('LunarLander-v3', render_mode='human')  # continuous: LunarLanderContinuous-v3\n",
    "env.reset()\n",
    "\n",
    "model = PPO('MlpPolicy', env, verbose=1, device='cpu')\n",
    "model.learn(total_timesteps=100000)\n",
    "\n",
    "episodes = 5\n",
    "\n",
    "for ep in range(episodes):\n",
    "    observation, info = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _states = model.predict(observation)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        env.render()\n",
    "        print(reward)"
   ],
   "id": "c3fcc14afb83b791",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "It's that simple to try PPO instead! After 100K steps with PPO:\n",
    "\n",
    "```text\n",
    "-----------------------------------------\n",
    "| rollout/                |             |\n",
    "|    ep_len_mean          | 575         |\n",
    "|    ep_rew_mean          | -0.463      |\n",
    "| time/                   |             |\n",
    "|    fps                  | 468         |\n",
    "|    iterations           | 49          |\n",
    "|    time_elapsed         | 214         |\n",
    "|    total_timesteps      | 100352      |\n",
    "| train/                  |             |\n",
    "|    approx_kl            | 0.013918768 |\n",
    "|    clip_fraction        | 0.134       |\n",
    "|    clip_range           | 0.2         |\n",
    "|    entropy_loss         | -0.92       |\n",
    "|    explained_variance   | 0.305       |\n",
    "|    learning_rate        | 0.0003      |\n",
    "|    loss                 | 73.1        |\n",
    "|    n_updates            | 480         |\n",
    "|    policy_gradient_loss | -0.00609    |\n",
    "|    value_loss           | 76.8        |\n",
    "-----------------------------------------\n",
    "```\n",
    "\n",
    "This agent looks much better, and it seems to be better overall.\n",
    "\n",
    "At this point, you should have a very general idea of how to use Stable Baslines 3 and some of how reinforcement learning works."
   ],
   "id": "cde3a4840381e1b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now try DQN, with a Neural Network and a CUDA device, if available. All we need to do is import it:\n",
    "\n",
    "```python\n",
    "from stable_baselines3 import DQN\n",
    "import torch\n",
    "```\n",
    "\n",
    "Then change our model to DQN:\n",
    "\n",
    "```python\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DQN('MlpPolicy', env, verbose=1, device=device)\n",
    "```\n"
   ],
   "id": "7f3d90d468a31608"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "import torch\n",
    "\n",
    "\n",
    "env = gym.make('LunarLander-v3', render_mode='human')  # continuous: LunarLanderContinuous-v3\n",
    "env.reset(seed=42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DQN('MlpPolicy', env, verbose=1, device=device)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "for ep in range(episodes):\n",
    "    observation, info = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _states = model.predict(observation)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        env.render()\n",
    "        print(reward)"
   ],
   "id": "b10294dd347b2dde",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "566c84c0eaa819d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# How to save and load models\n",
    "\n",
    "This is the part 2 of the reinforcement learning with Stable Baselines 3 tutorials. We left off with training a few models in the lunar lander environment. While this was beginning to work, it seemed like maybe even more training would help. How much more? When we trained for 10,000 steps, and decided we wanted to try 100,000 steps, we had to start all over. If we want to try 1,000,000 steps, we'll also need to start over. It makes a lot more sense to save models along the way, and to probably just train until you're happy with the model or want to change it in some way, which will require starting over. With this in mind, how might we save and load models?\n",
    "\n",
    "Our training code up to this point:"
   ],
   "id": "20232edf6d527716"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "env = gym.make('LunarLander-v3', render_mode='human')\n",
    "env.reset()\n",
    "\n",
    "model = PPO('MlpPolicy', env, verbose=1, device='cpu')\n",
    "model.learn(total_timesteps=100000)"
   ],
   "id": "544921aba633e4a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Let's decrease the timesteps to 10,000 instead, as well as create a models directory:\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "\n",
    "models_dir = \"models/PPO\"\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "```\n",
    "\n",
    "Then, we'll encase our training in a while loop:\n",
    "\n",
    "```python\n",
    "TIMESTEPS = 10000\n",
    "while True:\n",
    "    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False)\n",
    "```\n",
    "\n",
    "Note the `reset_num_timesteps=False`. This allows us to see the actual total number of timesteps for the model rather than resetting every iteration. We're also setting a constant for however many timesteps we want to do per iteration. Now we can save with `mode.save(PATH)`. Let's track however many iterations we've made, and then calculate what timestep that model is, saving that as the name:\n",
    "\n",
    "```python\n",
    "TIMESTEPS = 10000\n",
    "iters = 0\n",
    "while True:\n",
    "    iters += 1\n",
    "    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False)\n",
    "    model.save(f\"{models_dir}/{TIMESTEPS*iters}\")\n",
    "```\n",
    "\n",
    "Full code up to this point:"
   ],
   "id": "5e9ef9260d0f5b5d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "import os\n",
    "\n",
    "\n",
    "models_dir = \"models/PPO\"\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "\n",
    "env = gym.make('LunarLander-v3', render_mode=None)\n",
    "env.reset()\n",
    "\n",
    "model = PPO('MlpPolicy', env, verbose=1, device='cpu')\n",
    "\n",
    "TIMESTEPS = 10000\n",
    "iters = 0\n",
    "while True:\n",
    "    iters += 1\n",
    "\n",
    "    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False)\n",
    "    model.save(f\"{models_dir}/{TIMESTEPS*iters}\")"
   ],
   "id": "7b9bfeacc03919fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Let's go ahead and run that for a bit, you should see every ~10,000 timesteps a model will be saved. **Keep in mind that this cell runs with an infinite loop. Interrupt it whenever you want.** You can just keep an eye on the `ep_rew_mean`, waiting for something positive, or just go make some coffee or something while you wait for the model to hopefully do something. Around 110,000 steps, the agent began scoring an average in the positives:\n",
    "\n",
    "```text\n",
    "------------------------------------------\n",
    "| rollout/                |              |\n",
    "|    ep_len_mean          | 628          |\n",
    "|    ep_rew_mean          | 4.46         |\n",
    "| time/                   |              |\n",
    "|    fps                  | 6744         |\n",
    "|    iterations           | 4            |\n",
    "|    time_elapsed         | 16           |\n",
    "|    total_timesteps      | 110592       |\n",
    "| train/                  |              |\n",
    "|    approx_kl            | 0.0025332319 |\n",
    "|    clip_fraction        | 0.0189       |\n",
    "|    clip_range           | 0.2          |\n",
    "|    entropy_loss         | -0.975       |\n",
    "|    explained_variance   | 0.759        |\n",
    "|    learning_rate        | 0.0003       |\n",
    "|    loss                 | 2            |\n",
    "|    n_updates            | 530          |\n",
    "|    policy_gradient_loss | -0.00157     |\n",
    "|    value_loss           | 29.1         |\n",
    "------------------------------------------\n",
    "```\n",
    "\n",
    "By 150,000 steps, the model appears to be on to something:\n",
    "\n",
    "```text\n",
    "-----------------------------------------\n",
    "| rollout/                |             |\n",
    "|    ep_len_mean          | 783         |\n",
    "|    ep_rew_mean          | 80.6        |\n",
    "| time/                   |             |\n",
    "|    fps                  | 9801        |\n",
    "|    iterations           | 4           |\n",
    "|    time_elapsed         | 15          |\n",
    "|    total_timesteps      | 151552      |\n",
    "| train/                  |             |\n",
    "|    approx_kl            | 0.013495723 |\n",
    "|    clip_fraction        | 0.0808      |\n",
    "|    clip_range           | 0.2         |\n",
    "|    entropy_loss         | -0.723      |\n",
    "|    explained_variance   | 0.719       |\n",
    "|    learning_rate        | 0.0003      |\n",
    "|    loss                 | 4.06        |\n",
    "|    n_updates            | 730         |\n",
    "|    policy_gradient_loss | -0.00426    |\n",
    "|    value_loss           | 48.3        |\n",
    "-----------------------------------------\n",
    "```\n",
    "\n",
    "We have these models saved, so we might as well let things continue to train as long as the model is improving quickly. While we wait, we can start programming the code required to load and actually run the model, so we can see it visually. In a separate script, the main bit of code for loading a model is:\n",
    "\n",
    "```python\n",
    "env = gym.make('LunarLander-v3', render_mode='human')  # continuous: LunarLanderContinuous-v3\n",
    "env.reset()\n",
    "\n",
    "model_path = f\"{models_dir}/250000.zip\"\n",
    "model = PPO.load(model_path, env=env, device='cpu')\n",
    "```\n",
    "\n",
    "Then, we can use the same code as we used before to play the model from part one, making our full code:"
   ],
   "id": "39498f521c9e77e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "models_dir = \"models/PPO\"\n",
    "\n",
    "env = gym.make('LunarLander-v3', render_mode='human')  # continuous: LunarLanderContinuous-v3\n",
    "env.reset()\n",
    "\n",
    "model_path = f\"{models_dir}/250000.zip\"\n",
    "model = PPO.load(model_path, env=env, device='cpu')\n",
    "\n",
    "episodes = 5\n",
    "\n",
    "for ep in range(episodes):\n",
    "    observation, info = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _states = model.predict(observation)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        env.render()\n",
    "        print(reward)"
   ],
   "id": "2b1512473a91729f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This appears to be \"solved\" at this point, and it only took a few minutes! It wont always be this easy, but you can probably stop training this model now. On a more realistic problem, you will likely need to go for millions...or billions... of steps, and that's if things work at all. You'll also probably need to tweak things like the reward function. But, for now, let's take the win!\n",
    "\n",
    "While we can see our model's stats in the console, it can often be hard to really visualize where you are in the training process, and it can be even harder when you're going to be comparing multiple models. The next thing we can do to make this easier is to log model performance with Tensorboard. To do this, we just need to specify a name and location for the Tensorboard logs. First, we'll make sure the log dir exists:\n",
    "\n",
    "```python\n",
    "logdir = \"logs\"\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)\n",
    "```\n",
    "\n",
    "Next, when specifying the model, we can pass the log directory:\n",
    "\n",
    "```python\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=logdir)\n",
    "```\n",
    "\n",
    "As we train, we can name the individual log:\n",
    "\n",
    "```python\n",
    "model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"PPO\")\n",
    "```\n",
    "\n",
    "Finally, let's limit the number of steps to 300K. Making the full code now:"
   ],
   "id": "bdeb01344dec1f4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "import os\n",
    "\n",
    "\n",
    "models_dir = \"models/PPO\"\n",
    "logdir = \"logs\"\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)\n",
    "\n",
    "env = gym.make('LunarLander-v3', render_mode=None)\n",
    "env.reset()\n",
    "\n",
    "model = PPO('MlpPolicy', env, verbose=1, device='cpu', tensorboard_log=logdir)\n",
    "\n",
    "TIMESTEPS = 10000\n",
    "iters = 0\n",
    "for i in range(30):\n",
    "    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"PPO\")\n",
    "    model.save(f\"{models_dir}/{TIMESTEPS*i}\")"
   ],
   "id": "5c5a5dd68e466ff8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now, while the model trains, we can view the results over time by opening a new terminal and doing: `tensorboard --logdir=logs`, and point your browser to [http://localhost:6006](http://localhost:6006)\n",
    "\n",
    "Now we can compare to other algorithms to see which one performs better. Let's try A2C again by changing the import, model definition, and log name:"
   ],
   "id": "cb12ba910a682f24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import A2C\n",
    "import os\n",
    "\n",
    "\n",
    "models_dir = \"models/A2C\"\n",
    "logdir = \"logs\"\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)\n",
    "\n",
    "env = gym.make('LunarLander-v3', render_mode=None)\n",
    "env.reset()\n",
    "\n",
    "model = A2C('MlpPolicy', env, verbose=1, device='cpu', tensorboard_log=logdir)\n",
    "\n",
    "TIMESTEPS = 10000\n",
    "iters = 0\n",
    "for i in range(30):\n",
    "    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"A2C\")\n",
    "    model.save(f\"{models_dir}/{TIMESTEPS*i}\")"
   ],
   "id": "619da18f41adb33e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, it should be much more clear the differences between PPO and A2C. The most obvious difference is A2C seems far more volatile in the reward over time.\n",
   "id": "5eb24acb4433e92b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
